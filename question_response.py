# -*- coding: utf-8 -*-
"""Question/Response.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U9zwEOx9pAMosmVvIQyB2Uk_qciUXksC
"""

!pip install transformers==2.9.0
!nvidia-smi

import random
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup
)
def set_seed(seed):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
set_seed(42)

tokenizer = T5Tokenizer.from_pretrained('t5-base')
t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')

# optimizer
no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters = [
    {
        "params": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],
        "weight_decay": 0.0,
    },
    {
        "params": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],
        "weight_decay": 0.0,
    },
]
optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)

true_false_adjective_tuples = [
                               (("Question: Can you lend me some money?",	"Response: I'm completely broke until payday."), "no"), 
                               (("Question: Do you think he did the theft?",	"Response: He is as innocent as a lamb"), "no"),
                               (("Question: Are you sure you want the spoiler",	"Response: haha sure. by the time, I get to it; I will forget the details, only knowing that it's touching at the end"),"yes"),
                               (("Question: Does tomorrow afternoon work for you?",	"Response: Yeah tomorrow afternoon works for me."),	"yes"),
                               (("Question: oh wow, Cuban cigar? I've never smoke before",	"Response: This one is actually from Peru, which is more my taste."),	"no"),
                               (("Question: Are you sending them an invitation?",	"Response: Certainly."),	"yes"),
                               (("Question: You, uh, you don't want to go for a ride, do you?",	"Response: Is it safe?"),	'yes'),
                               (("Question: Have you ever made them yourself?",	"Response: no. only eat them"),	"no"),
                               (("Question: Have you seen my watch?",	"Response: I will take a look for it around my house."),	"no"),
                               (("Question: You don't look normal. Are you all right?", "Response: I have a headache."),	"no"),
                               (("Question: any improvements in your marathon time?",	"Response: No, only a few minutes. I still find it difficult to run long races"),	"no"),
                               (("Question: do you live near San Mateo?",	"Response: no, but I'm willing to drive for good ramen"),	"no"),
                               (("Question: Can he play the violin?",	"Response: Can a pig fly?"),	"no"),
                               (("Question: Aren't you scared?", "Response: Sometimes."),	"yes")
]

t5_model.train()

epochs = 10

for epoch in range(epochs):
  print ("epoch ",epoch)
  for input,output in true_false_adjective_tuples:
    input_sent = "Answer 'yes' or 'no' based on the intention of the response: "+input[0] + input[1]+ " </s>"
    ouput_sent = output+" </s>"

    tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=96, pad_to_max_length=True,return_tensors="pt")
    tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=96, pad_to_max_length=True,return_tensors="pt")


    input_ids  = tokenized_inp["input_ids"]
    attention_mask = tokenized_inp["attention_mask"]

    lm_labels= tokenized_output["input_ids"]
    decoder_attention_mask=  tokenized_output["attention_mask"]


    # the forward function automatically creates the correct decoder_input_ids
    output = t5_model(input_ids=input_ids, lm_labels=lm_labels,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)
    loss = output[0]

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

import csv
file = csv.reader(open('testing.csv'))
correct = 0
wrong = 0
total = 0
tp = 0
tn = 0
fp = 0
fn = 0
for line in file:
  test_sent = "Answer 'yes' or 'no' based on the intention of the response: Question: " + line[0][:line[0].find("?")+1] + " Response: " + line[0][line[0].find("?")+1:] + "</s>"
  test_tokenized = tokenizer.encode_plus(test_sent, return_tensors="pt")
  print(test_sent)
  test_input_ids  = test_tokenized["input_ids"]
  test_attention_mask = test_tokenized["attention_mask"]

  t5_model.eval()
  beam_outputs = t5_model.generate(
      input_ids=test_input_ids,attention_mask=test_attention_mask,
      max_length=64,
      early_stopping=True,
      num_beams=10,
      num_return_sequences=1,
      no_repeat_ngram_size=2
  )

  for beam_output in beam_outputs:
      sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
      print("Prediction and Answer")
      print(sent, line[1])
      if sent == line[1]:
        correct += 1
      else:
        wrong += 1
      if sent == 'yes' and line[1] == 'yes':
        tp += 1
      elif sent == 'yes' and line[1] == 'no':
        fp += 1
      elif sent == 'no' and line[1] == 'yes':
        fn += 1
      elif sent == 'no' and line[1] == 'no':
        tn += 1
      print(correct, wrong)
  print("")
  total += 1
  print(total)

print("Correct Predictions: ",correct)
print("Incorrect Predictions:", wrong)
print("Accuracy:", correct/total)

recall = tp / (tp + fn)
precision = tp / (tp + fp)
f1_score = (2 * recall * precision) / (recall + precision) 
print("Recall: ", recall)
print("Precision: ", precision)
print("F1 score: ", f1_score)

